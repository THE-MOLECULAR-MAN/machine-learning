{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing a Machine Learning model for detecting Internet of Things Malware\n",
    "Based off AGUNG PAMBUDI's dataset published on Kaggle: https://www.kaggle.com/datasets/agungpambudi/network-malware-detection-connection-analysis/data\n",
    "\n",
    "## Primary objectives:\n",
    "* develop a well generalized model with a targeted (TBD Metric) of (TBD metric value)\n",
    "* learn how to load/process data at gigabyte scale with on-prem hardware\n",
    "\n",
    "## To Do Lists:\n",
    "### To Do - Optimization / Performance\n",
    "* switch input file from CSV to Parquet, partition data on disk\n",
    "* switch output from from CSV to Parquet\n",
    "* minimize memory utilization - select smallest appropriate datatypes for each feature\n",
    "* add support for multi-threading and vectorization, especially my custom feature functions\n",
    "* display progress meter for reading/writing files if possible, maybe ETA too\n",
    "\n",
    "### To Do - Data processing\n",
    "* check for bias in data, rebalance as needed\n",
    "* winsorize selected numerical features\n",
    "* normalize selected numerical features\n",
    "* investigate feature correlation with other features and the label, remove features that aren't providing value or that are too correlated with other features\n",
    "* display counts of missing values as a percentage of the whole\n",
    "* double check tunnel_parents column for unique values \n",
    "* Figure out what to do with missing values - remove or replace\n",
    "* OHE - get count of # of new columns created during process, display it\n",
    "* OHE - make sure it is not one-hot encoding Booleans\n",
    "\n",
    "### To Do - Feature enhancement ideas\n",
    "* check for service does NOT match the port/protocol because that is something suspicious and implies they're trying to hide something. Initial version: use hardcoded dict to support the 5-6 services listed. Enhanced: use third party service to do the lookup and support multiple port numbers for a single service (like http)\n",
    "* add support for threat intelligence feeds wrt IPs and other stuff\n",
    "\n",
    "### To Do - Research:\n",
    "* investigate other common Intrusion Detection System Indictors of Comprimise for possible feature development\n",
    "* investigate that latest MITRE ATT&CK framework methodologies and attacker trends for possible feature development\n",
    "\n",
    "### To Do - Modeling:\n",
    "* Build a Jupyter Notebook for training & optimizing Logistic Regression models on this data\n",
    "* Optimize the above model using gradient descent on hyperparameters\n",
    "* try out other model algorithms like decision tree, and others\n",
    "\n",
    "### To Do -  Longer term:\n",
    "* apply the model to my existing data lake of netflow, DNS, endpoint collection, and other data from SIEM.\n",
    "* implement model training as a pipeline to automatically create and optimize new models, roll them out\n",
    "* deploy the model to my local network and scan network traffic in near real time\n",
    "* investigate running in parallel computing, cloud providers, and/or GPU\n",
    "\n",
    "Misc notes:\n",
    "* Currently uses 8-9 GB of working set RAM, peaks around 12 GB during loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance improvements\n",
    "# %pip install cython\n",
    "# %pip install numba\n",
    "%load_ext Cython\n",
    "import numba\n",
    "numba.set_num_threads(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ipaddress\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "# import re\n",
    "# from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geoip2.database\n",
    "import humanize\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# sns.set_theme()\n",
    "# from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_prefix = './data/CTU-IoT-Malware-Capture'\n",
    "csv_files_to_load = glob.glob('./data/CTU-IoT-Malware-Capture*.labeled.csv')\n",
    "training_outfile = output_file_prefix + \"_train.csv\"\n",
    "\n",
    "ORIGINAL_LABEL_COLUMN_NAME = 'label'\n",
    "LABEL_COLUMN_NAME = 'label_bool'\n",
    "\n",
    "NORMALIZE_METHOD = \"min_max\"\n",
    "\n",
    "COLUMN_NAMES_CATEGORICAL = [ #'ip_asn', 'ip_dest_country',\n",
    "                            'id.resp_p', 'id.orig_p',\n",
    "                            'id.orig_h', 'id.resp_h',\n",
    "                            'proto', 'service', 'conn_state']\n",
    "\n",
    "# https://stackoverflow.com/questions/29245848/what-are-all-the-dtypes-that-pandas-recognizes\n",
    "FEATURE_PROPER_DATATYPES = {\n",
    "    'local_orig':   'Int64',\n",
    "    'local_resp':   'Int64',\n",
    "    'missed_bytes': 'Int64',\n",
    "    'id.resp_p':    'category',\n",
    "    'id.orig_p':    'category',\n",
    "    'id.orig_h':    'category',\n",
    "    'id.resp_h':    'category',\n",
    "    'proto':        'category',\n",
    "    'service':      'category',\n",
    "    'conn_state':   'category',\n",
    "    'tunnel_parents':   'category',\n",
    "    'duration':     'float32',   # np.float32\n",
    "    'history':      'category'\n",
    "    # 'ts': have to pass parameters\n",
    "    \n",
    "    #'orig_bytes':   int,       # has NaN values\n",
    "    #'resp_bytes':   int        # has NaN values\n",
    "    }\n",
    "\n",
    "columns_to_OHE = ['proto', 'service', 'conn_state', \n",
    "                  'history', 'ip_dest_country'] \n",
    "                    #'id.resp_h', 'id.orig_h']\n",
    "                    \n",
    "SERVICE_TO_PROTOCOL_AND_PORT_MAPPINGS = {\n",
    "  'ssh': {'protocol': 'tcp', 'port': 22},\n",
    "  'dns': {'protocol': 'udp', 'port': 53},\n",
    "}\n",
    "\n",
    "geoip_country = geoip2.database.Reader('./geoip/GeoLite2-Country_20240308/GeoLite2-Country.mmdb')\n",
    "geoip_asn     = geoip2.database.Reader('./geoip/GeoLite2-ASN_20240308/GeoLite2-ASN.mmdb')\n",
    "\n",
    "def get_human_friendly_mem_size(dfx):\n",
    "    return humanize.naturalsize(dfx.memory_usage(index=True, deep=True))\n",
    "    #  sys.getsizeof(objname))\n",
    "\n",
    "# df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data into a Pandas dataframe\n",
    "Define the path to the dataset file\n",
    "Define the name of the label column\n",
    "\n",
    "performance\n",
    "|Runtime|Parser Engine|specified datatypes|specified index|specified chunksize|memory_map|df.memory_usage|\n",
    "|---|---|---|---|---|---|---|\n",
    "|96 sec|unspecifed|unspecifed|unspecified|unspecified|unspecified (Default: False)|?|\n",
    "|3.5 min|unspecifed|almost all|uid|unspecified|unspecified (Default: False)|?|\n",
    "|4 min 6 sec|unspecifed|almost all|uid|unspecified|True|16.9 GB|\n",
    "|2 min 7 sec|pyarrow|almost all|uid|unspecified|True|14.3 GB|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a SINGLE CSV file:\n",
    "# rootdir = os.getcwd()\n",
    "# infile = os.path.join(rootdir, 'data',\n",
    "#                       'CTU-IoT-Malware-Capture-20-1conn.log.labeled.csv')\n",
    "# df = pd.read_csv(infile, delimiter='|', na_values='-')\n",
    "\n",
    "# load a directory of CSV files:\n",
    "\n",
    "# chunksizeint\n",
    "\n",
    "dfs = []\n",
    "for iter_csv_file in csv_files_to_load:\n",
    "    # filesize_MB = int(os.stat(iter_csv_file).st_size / (1024 * 1024))\n",
    "    # if filesize_MB >= 5:\n",
    "    #     print(f'skipping file {filesize_MB}, too big at {filesize_MB} MB')\n",
    "    #     continue\n",
    "    # /var/folders/90/cd8pt9qd43q0svfjsljg9ccr0000gn/T/ipykernel_60137/4122006169.py:16: DtypeWarning: Columns (7,22) have mixed types. Specify dtype option on import or set low_memory=False.\n",
    "\n",
    "    # https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#io-chunking\n",
    "    # https://stackoverflow.com/questions/66346343/can-i-load-multiple-csv-files-using-pyarrow\n",
    "    # https://arrow.apache.org/docs/2.0/python/generated/pyarrow.csv.ReadOptions.html#pyarrow.csv.ReadOptions\n",
    "    \n",
    "    df_temp = pd.read_csv(iter_csv_file,\n",
    "                          index_col='uid',\n",
    "                          engine='pyarrow',\n",
    "                          dtype=FEATURE_PROPER_DATATYPES,\n",
    "                          delimiter='|',\n",
    "                          na_values='-'\n",
    "    )\n",
    "    dfs.append(df_temp)\n",
    "    del df_temp\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "del dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index                    132\n",
       "ts                 200088024\n",
       "id.orig_h         1765233049\n",
       "id.orig_p          200088024\n",
       "id.resp_h         1767415304\n",
       "id.resp_p          200088024\n",
       "proto             1500685296\n",
       "service           1450407097\n",
       "duration           100044012\n",
       "orig_bytes         200088024\n",
       "resp_bytes         200088024\n",
       "conn_state        1487835090\n",
       "local_orig         225099027\n",
       "local_resp         225099027\n",
       "missed_bytes       225099027\n",
       "history           1450932403\n",
       "orig_pkts          200088024\n",
       "orig_ip_bytes      200088024\n",
       "resp_pkts          200088024\n",
       "resp_ip_bytes      200088024\n",
       "tunnel_parents      25011111\n",
       "label             1759724500\n",
       "detailed-label    1609753404\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.memory_usage.html\n",
    "# df.memory_usage(deep=True)\n",
    "# get_human_friendly_mem_size(df)\n",
    "df.memory_usage(deep=True)\n",
    "# humanize.naturalsize(1000000)\n",
    "# get_human_friendly_mem_size(1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customized variables for this dataset\n",
    "\n",
    "Feature description from documentation: https://www.kaggle.com/datasets/agungpambudi/network-malware-detection-connection-analysis/data\n",
    "\n",
    "|Field Name|Description|Type|\n",
    "| ----------- | ----------- | ----------- |\n",
    "|ts|The timestamp of the connection event.|time|\n",
    "|uid|A unique identifier for the connection.|string|\n",
    "|id.orig_h|The source IP address.|addr|\n",
    "|id.orig_p|The source port.|port|\n",
    "|id.resp_h|The destination IP address.|addr|\n",
    "|id.resp_p|The destination port.|port|\n",
    "|proto|The network protocol used (e.g., 'tcp').|enum|\n",
    "|service|The service associated with the connection.|string|\n",
    "|duration|The duration of the connection.|interval|\n",
    "|orig_bytes|The number of bytes sent from the source to the destination.|count|\n",
    "|resp_bytes|The number of bytes sent from the destination to the source.|count|\n",
    "|conn_state|The state of the connection.|string|\n",
    "|local_orig|Indicates whether the connection is considered local or not.|bool|\n",
    "|local_resp|Indicates whether the connection is considered local or not.|bool|\n",
    "|missed_bytes|The number of missed bytes in the connection.|count|\n",
    "|history|A history of connection states.|string|\n",
    "|orig_pkts|The number of packets sent from the source to the destination.|count|\n",
    "|orig_ip_bytes|The number of IP bytes sent from the source to the destination.|count|\n",
    "|resp_pkts|The number of packets sent from the destination to the source.|count|\n",
    "|resp_ip_bytes|The number of IP bytes sent from the destination to the source.|count|\n",
    "|tunnel_parents|Indicates if this connection is part of a tunnel.|set[string]|\n",
    "|label|A label associated with the connection (e.g., 'Malicious' or 'Benign').|string|\n",
    "|detailed-label|A more detailed description or label for the connection.|string|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['service'].unique()\n",
    "# array([nan, 'dns', 'http', 'dhcp', 'ssl', 'irc', 'ssh'], dtype=object)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforms\n",
    "\n",
    "More transform ideas:\n",
    "* service vs port/protcol mismatch\n",
    "* first time contact between client/server\n",
    "* receiving end high port\n",
    "* total last 24 hour bandwidth between client/server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the label as boolean\n",
    "df[LABEL_COLUMN_NAME] = df[ORIGINAL_LABEL_COLUMN_NAME].isin(['Malicious   C&C']).astype(int).astype(bool)\n",
    "\n",
    "for colname, newdatatype in FEATURE_PROPER_DATATYPES.items():\n",
    "    df[colname] = df[colname].astype(newdatatype)\n",
    "\n",
    "# for iter_colname in COLUMN_NAMES_CATEGORICAL:\n",
    "#     df[iter_colname] = df[iter_colname].astype('category')\n",
    "   \n",
    "\n",
    "df['is_tunneled'] = not(df['tunnel_parents'].isna)\n",
    "\n",
    "# converting the date to timestamp,\n",
    "# need the unit='s' to convert Unix time\n",
    "df['ts_converted'] = pd.to_datetime(\n",
    "    df['ts'], errors=\"raise\",\n",
    "    unit='s'\n",
    ")\n",
    "\n",
    "df.set_index('uid', inplace=True)\n",
    "# IP_ADDRESS_COLUMN_NAMES = ['id.orig_h', 'id.resp_h']\n",
    "# for iter_colname in IP_ADDRESS_COLUMN_NAMES:\n",
    "#     df[iter_colname] = df[iter_colname].apply(ipaddress.ip_address)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some sample data after the transforms\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locating missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Locating missing values:\n",
    "nan_count = np.sum(df.isnull(), axis=0).sort_values(ascending=False)\n",
    "\n",
    "# display just columns that have at least 1 missing value:\n",
    "nan_count[nan_count > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing columns that the model doesn't use\n",
    "TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\n",
    "    columns=[\n",
    "        ORIGINAL_LABEL_COLUMN_NAME,     # was replaced\n",
    "        \"detailed-label\",               # will be used in future version of this Notebook\n",
    "        \"ts\",                           # was converted to a new column\n",
    "        # \"uid\",                           # unique identifier, not used by model\n",
    "        \"tunnel_parents\"                # documentation isn't clear enough on what this is or how it is formatted or why to be useful\n",
    "    ],\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformations - Data Enrichment via adding features\n",
    "\n",
    "This step takes the longest - about 8 minutes on a MacBook air w/o vectorizing via numba or specifying # of threads via "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure and load the GeoIP databases\n",
    "# %pip install geoip2\n",
    "# restart the kernel\n",
    "\n",
    "# https://dev.maxmind.com/geoip/geolite2-free-geolocation-data?lang=en  \n",
    "# https://www.maxmind.com/en/accounts/985797/geoip/downloads\n",
    "# https://github.com/maxmind/GeoIP2-python?tab=readme-ov-file#database-usage\n",
    "\n",
    "# @numba.vectorize\n",
    "def ip_to_country(ip_as_str):\n",
    "    try:\n",
    "        ip = ipaddress.ip_address(ip_as_str)\n",
    "        if ip.is_global:\n",
    "            return geoip_country.country(ip).country.name\n",
    "    finally:\n",
    "        return None\n",
    "\n",
    "# @numba.vectorize\n",
    "def ip_to_asn(ip_as_str):\n",
    "    try:\n",
    "        ip = ipaddress.ip_address(ip_as_str)\n",
    "        if ip.is_global:\n",
    "            return geoip_asn.asn(ip).autonomous_system_number\n",
    "    finally:\n",
    "        return None\n",
    "\n",
    "# GeoIP\n",
    "df['ip_dest_country'] = df['id.resp_h'].apply(ip_to_country)\n",
    "df['ip_asn']          = df['id.resp_h'].apply(ip_to_asn)\n",
    "\n",
    "print(df['ip_dest_country'].unique().tolist())\n",
    "print(df['ip_asn'].unique().tolist())\n",
    "\n",
    "\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting strings to one-hot encoded columns\n",
    "Locate string columns that have a small number of unique values and replace them with one-hot encoded versions, then remove the original column.\n",
    "\n",
    "Runtime: 3 min on Macbook air"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter_column_name in columns_to_OHE:\n",
    "    # define a new column name\n",
    "    new_column_prefix = iter_column_name # + '_onehot_'\n",
    "    \n",
    "    # create a one-hot encoded version in a new dataframe\n",
    "    temp_df = pd.get_dummies(df[iter_column_name], prefix=new_column_prefix)\n",
    "\n",
    "    # merge the new dataframe into the existing one\n",
    "    df = df.join(temp_df)\n",
    "\n",
    "    # remove the original column now that it has been encoded \n",
    "    # into the existing dataframe\n",
    "    df.drop(columns=iter_column_name, inplace=True)\n",
    "    \n",
    "    \n",
    "    print(f'One-hot encoded: {iter_column_name} into {new_column_prefix}*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Everything should be reduced to numbers at this point\n",
    "\n",
    "list_of_string_columns = df.select_dtypes(include=object).columns.tolist()\n",
    "\n",
    "# create a Pandas Series that lists the string columns by ascending counts\n",
    "df_unique_string_vals = df[list_of_string_columns].nunique().sort_values(ascending=True)\n",
    "df_unique_string_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-order the columns\n",
    "Sort the column names alphabetically, but make sure the 'label' column is always last.\n",
    "AWS Sagemaker cares about the order and having the label be last."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alphabetically sort the column names, but leave the label as the last column\n",
    "column_order = sorted(df.columns)\n",
    "column_order.remove(LABEL_COLUMN_NAME)\n",
    "column_order.append(LABEL_COLUMN_NAME)\n",
    "df = df.reindex(column_order, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime: 1 min on Macbook Air\n",
    "# check for missing values\n",
    "# check for any remaining strings\n",
    "#df.describe(include=\"all\")\n",
    "\n",
    "pd.describe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the final datatypes before exporting to CSV\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close the readers\n",
    "geoip_country.close()\n",
    "geoip_asn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# size output\n",
    "\n",
    "# print(df.size) # total number of cells (rows times columns)\n",
    "print(df.shape[0])\n",
    "\n",
    "# print(humanize.naturalsize(sys.getsizeof(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing training and prediction data into CSV files\n",
    "\n",
    "full dataframe has 25011003 rows\n",
    "\n",
    "|file type|# of rows|size|runtime|filename|expanded file size (MB)|compression ratio (%)|% of data exported|Est runtime for full data|Est full file size|\n",
    "|---|---|---|---|---|---|---|---|---|---|\n",
    "|CSV|509191|898 MB|?|CTU-IoT-Malware-Capture_train.csv|same|N/A|2.0%|?|43 GB|\n",
    "|XZ|379831|12 MB|10 min 36 sec|CTU-IoT-Malware-Capture_train.xz|703 MB|98%|1.5186%|11.5 hours|790 MB|\n",
    "|CSV|40261|74.5 MB|60 sec|CTU-IoT-Malware-Capture_train.csv|same|N/A|0.001609731524961|10 hours |45 GB|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime on Macbook air with full dataset to uncompressed file: at least 8 min\n",
    "#   CSV is mariginally faster than XZ, but takes up way more space\n",
    "#   https://dask.pydata.org/en/latest/diagnostics-local.html\n",
    "#   increase # of rows/block size\n",
    "#   https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_csv.html\n",
    "\n",
    "\n",
    "# Create a training/test dataset and output to CSV\n",
    "\n",
    "df.to_csv(training_outfile)\n",
    "print(f\"Training data saved to new CSV file:\\n{training_outfile}\")\n",
    "\n",
    "output_filesize = humanize.naturalsize(os.stat(training_outfile).st_size)\n",
    "print(f'output file size: {output_filesize}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
